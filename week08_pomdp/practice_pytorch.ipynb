{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from IPython.core import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "# if you're running in colab\n",
    "# !wget https://raw.githubusercontent.com/yandexdataschool/Practical_RL/0ccb0673965dd650d9b284e1ec90c2bfd82c8a94/week08_pomdp/atari_util.py\n",
    "# !wget https://raw.githubusercontent.com/yandexdataschool/Practical_RL/0ccb0673965dd650d9b284e1ec90c2bfd82c8a94/week08_pomdp/env_pool.py\n",
    "\n",
    "# If you are running on a server, launch xvfb to record game videos\n",
    "# Please make sure you have xvfb installed\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY = : 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kung-Fu, recurrent style\n",
    "\n",
    "In this notebook we'll once again train RL agent for for atari [KungFuMaster](https://gym.openai.com/envs/KungFuMaster-v0/), this time using recurrent neural networks.\n",
    "\n",
    "![http://www.retroland.com/wp-content/uploads/2011/07/King-Fu-Master.jpg](http://www.retroland.com/wp-content/uploads/2011/07/King-Fu-Master.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation shape: (1, 42, 42)\n",
      "Num actions: 14\n",
      "Action names: ['NOOP', 'UP', 'RIGHT', 'LEFT', 'DOWN', 'DOWNRIGHT', 'DOWNLEFT', 'RIGHTFIRE', 'LEFTFIRE', 'DOWNFIRE', 'UPRIGHTFIRE', 'UPLEFTFIRE', 'DOWNRIGHTFIRE', 'DOWNLEFTFIRE']\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from atari_util import PreprocessAtari\n",
    "\n",
    "\n",
    "def make_env():\n",
    "    env = gym.make(\"KungFuMasterDeterministic-v0\")\n",
    "    env = PreprocessAtari(env, height=42, width=42,\n",
    "                          crop=lambda img: img[60:-30, 15:],\n",
    "                          color=False, n_frames=1)\n",
    "    return env\n",
    "\n",
    "\n",
    "env = make_env()\n",
    "\n",
    "obs_shape = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(\"Observation shape:\", obs_shape)\n",
    "print(\"Num actions:\", n_actions)\n",
    "print(\"Action names:\", env.env.env.get_action_meanings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s = env.reset()\n",
    "for _ in range(100):\n",
    "    s, _, _, _ = env.step(env.action_space.sample())\n",
    "\n",
    "plt.title('Game image')\n",
    "plt.imshow(env.render('rgb_array'))\n",
    "plt.show()\n",
    "\n",
    "plt.title('Agent observation')\n",
    "plt.imshow(s.reshape([42, 42]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POMDP setting\n",
    "\n",
    "The atari game we're working with is actually a POMDP: your agent needs to know timing at which enemies spawn and move, but cannot do so unless it has some memory. \n",
    "\n",
    "Let's design another agent that has a recurrent neural net memory to solve this. Here's a sketch.\n",
    "\n",
    "![img](img1.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# a special module that converts [batch, channel, w, h] to [batch, units]\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SimpleRecurrentAgent(nn.Module):\n",
    "    def __init__(self, obs_shape, n_actions, reuse=False):\n",
    "        \"\"\"A simple actor-critic agent\"\"\"\n",
    "        super(self.__class__, self).__init__()\n",
    "\n",
    "        self.conv0 = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2))\n",
    "        self.conv1 = nn.Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2))\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2))\n",
    "        self.flatten = Flatten()\n",
    "\n",
    "        self.hid = nn.Linear(512, 128)\n",
    "        self.rnn = nn.LSTMCell(128, 128)\n",
    "\n",
    "        self.logits = nn.Linear(128, n_actions)\n",
    "        self.state_value = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, prev_state, obs_t):\n",
    "        \"\"\"\n",
    "        Takes agent's previous step and observation, \n",
    "        returns next state and whatever it needs to learn (tf tensors)\n",
    "        \"\"\"\n",
    "\n",
    "        # YOUR CODE: apply the whole neural net for one step here.\n",
    "        # See docs on self.rnn(...)\n",
    "        # the recurrent cell should take the last feedforward dense layer as input\n",
    "        <YOUR CODE >\n",
    "\n",
    "        new_state = <YOUR CODE >\n",
    "        logits = <YOUR CODE >\n",
    "        state_value = <YOUR CODE >\n",
    "\n",
    "        return new_state, (logits, state_value)\n",
    "\n",
    "    def get_initial_state(self, batch_size):\n",
    "        \"\"\"Return a list of agent memory states at game start. Each state is a np array of shape [batch_size, ...]\"\"\"\n",
    "        return torch.zeros((batch_size, 128)), torch.zeros((batch_size, 128))\n",
    "\n",
    "    def sample_actions(self, agent_outputs):\n",
    "        \"\"\"pick actions given numeric agent outputs (np arrays)\"\"\"\n",
    "        logits, state_values = agent_outputs\n",
    "        probs = F.softmax(logits)\n",
    "        return torch.multinomial(probs, 1)[:, 0].data.numpy()\n",
    "\n",
    "    def step(self, prev_state, obs_t):\n",
    "        \"\"\" like forward, but obs_t is a numpy array \"\"\"\n",
    "        obs_t = torch.tensor(np.asarray(obs_t), dtype=torch.float32)\n",
    "        (h, c), (l, s) = self.forward(prev_state, obs_t)\n",
    "        return (h.detach(), c.detach()), (l.detach(), s.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_parallel_games = 5\n",
    "gamma = 0.99\n",
    "\n",
    "agent = SimpleRecurrentAgent(obs_shape, n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state = [env.reset()]\n",
    "_, (logits, value) = agent.step(agent.get_initial_state(1), state)\n",
    "print(\"action logits:\\n\", logits)\n",
    "print(\"state values:\\n\", value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's play!\n",
    "Let's build a function that measures agent's average reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(agent, env, n_games=1):\n",
    "    \"\"\"Plays an entire game start to end, returns session rewards.\"\"\"\n",
    "\n",
    "    game_rewards = []\n",
    "    for _ in range(n_games):\n",
    "        # initial observation and memory\n",
    "        observation = env.reset()\n",
    "        prev_memories = agent.get_initial_state(1)\n",
    "\n",
    "        total_reward = 0\n",
    "        while True:\n",
    "            new_memories, readouts = agent.step(\n",
    "                prev_memories, observation[None, ...])\n",
    "            action = agent.sample_actions(readouts)\n",
    "\n",
    "            observation, reward, done, info = env.step(action[0])\n",
    "\n",
    "            total_reward += reward\n",
    "            prev_memories = new_memories\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        game_rewards.append(total_reward)\n",
    "    return game_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env_monitor = gym.wrappers.Monitor(env, directory=\"kungfu_videos\", force=True)\n",
    "rw = evaluate(agent, env_monitor, n_games=3,)\n",
    "env_monitor.close()\n",
    "print(rw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s: s.endswith(\n",
    "    \".mp4\"), os.listdir(\"./kungfu_videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./kungfu_videos/\"+video_names[-1]))  # this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on parallel games\n",
    "\n",
    "We introduce a class called EnvPool - it's a tool that handles multiple environments for you. Here's how it works:\n",
    "![img](img2.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from env_pool import EnvPool\n",
    "pool = EnvPool(agent, make_env, n_parallel_games)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We gonna train our agent on a thing called __rollouts:__\n",
    "![img](img3.jpg)\n",
    "\n",
    "A rollout is just a sequence of T observations, actions and rewards that agent took consequently.\n",
    "* First __s0__ is not necessarily initial state for the environment\n",
    "* Final state is not necessarily terminal\n",
    "* We sample several parallel rollouts for efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for each of n_parallel_games, take 10 steps\n",
    "rollout_obs, rollout_actions, rollout_rewards, rollout_mask = pool.interact(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Actions shape:\", rollout_actions.shape)\n",
    "print(\"Rewards shape:\", rollout_rewards.shape)\n",
    "print(\"Mask shape:\", rollout_mask.shape)\n",
    "print(\"Observations shape: \", rollout_obs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-critic objective\n",
    "\n",
    "Here we define a loss function that uses rollout above to train advantage actor-critic agent.\n",
    "\n",
    "\n",
    "Our loss consists of three components:\n",
    "\n",
    "* __The policy \"loss\"__\n",
    " $$ \\hat J = {1 \\over T} \\cdot \\sum_t { \\log \\pi(a_t | s_t) } \\cdot A_{const}(s,a) $$\n",
    "  * This function has no meaning in and of itself, but it was built such that\n",
    "  * $ \\nabla \\hat J = {1 \\over N} \\cdot \\sum_t { \\nabla \\log \\pi(a_t | s_t) } \\cdot A(s,a) \\approx \\nabla E_{s, a \\sim \\pi} R(s,a) $\n",
    "  * Therefore if we __maximize__ J_hat with gradient descent we will maximize expected reward\n",
    "  \n",
    "  \n",
    "* __The value \"loss\"__\n",
    "  $$ L_{td} = {1 \\over T} \\cdot \\sum_t { [r + \\gamma \\cdot V_{const}(s_{t+1}) - V(s_t)] ^ 2 }$$\n",
    "  * Ye Olde TD_loss from q-learning and alike\n",
    "  * If we minimize this loss, V(s) will converge to $V_\\pi(s) = E_{a \\sim \\pi(a | s)} R(s,a) $\n",
    "\n",
    "\n",
    "* __Entropy Regularizer__\n",
    "  $$ H = - {1 \\over T} \\sum_t \\sum_a {\\pi(a|s_t) \\cdot \\log \\pi (a|s_t)}$$\n",
    "  * If we __maximize__ entropy we discourage agent from predicting zero probability to actions\n",
    "  prematurely (a.k.a. exploration)\n",
    "  \n",
    "  \n",
    "So we optimize a linear combination of $L_{td}$ $- \\hat J$, $-H$\n",
    "  \n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "__One more thing:__ since we train on T-step rollouts, we can use N-step formula for advantage for free:\n",
    "  * At the last step, $A(s_t,a_t) = r(s_t, a_t) + \\gamma \\cdot V(s_{t+1}) - V(s) $\n",
    "  * One step earlier, $A(s_t,a_t) = r(s_t, a_t) + \\gamma \\cdot r(s_{t+1}, a_{t+1}) + \\gamma ^ 2 \\cdot V(s_{t+2}) - V(s) $\n",
    "  * Et cetera, et cetera. This way agent starts training much faster since it's estimate of A(s,a) depends less on his (imperfect) value function and more on actual rewards. There's also a [nice generalization](https://arxiv.org/abs/1506.02438) of this.\n",
    "\n",
    "\n",
    "__Note:__ it's also a good idea to scale rollout_len up to learn longer sequences. You may wish set it to >=20 or to start at 10 and then scale up as time passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(y, n_dims=None):\n",
    "    \"\"\" Take an integer tensor and convert it to 1-hot matrix. \"\"\"\n",
    "    y_tensor = y.to(dtype=torch.int64).view(-1, 1)\n",
    "    n_dims = n_dims if n_dims is not None else int(torch.max(y_tensor)) + 1\n",
    "    y_one_hot = torch.zeros(y_tensor.size()[0], n_dims).scatter_(1, y_tensor, 1)\n",
    "    return y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(agent.parameters(), lr=1e-5)\n",
    "\n",
    "\n",
    "def train_on_rollout(states, actions, rewards, is_not_done, prev_memory_states, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
    "    Updates agent's weights by following the policy gradient above.\n",
    "    Please use Adam optimizer with default parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    # shape: [batch_size, time, c, h, w]\n",
    "    states = torch.tensor(np.asarray(states), dtype=torch.float32)\n",
    "    actions = torch.tensor(np.array(actions), dtype=torch.int64)  # shape: [batch_size, time]\n",
    "    rewards = torch.tensor(np.array(rewards), dtype=torch.float32)  # shape: [batch_size, time]\n",
    "    is_not_done = torch.tensor(np.array(is_not_done), dtype=torch.float32)  # shape: [batch_size, time]\n",
    "    rollout_length = rewards.shape[1] - 1\n",
    "\n",
    "    # predict logits, probas and log-probas using an agent.\n",
    "    memory = [m.detach() for m in prev_memory_states]\n",
    "\n",
    "    logits = []  # append logit sequence here\n",
    "    state_values = []  # append state values here\n",
    "    for t in range(rewards.shape[1]):\n",
    "        obs_t = states[:, t]\n",
    "\n",
    "        # use agent to comute logits_t and state values_t.\n",
    "        # append them to logits and state_values array\n",
    "\n",
    "        memory, (logits_t, values_t) = <YOUR CODE >\n",
    "\n",
    "        logits.append(logits_t)\n",
    "        state_values.append(values_t)\n",
    "\n",
    "    logits = torch.stack(logits, dim=1)\n",
    "    state_values = torch.stack(state_values, dim=1)\n",
    "    probas = F.softmax(logits, dim=2)\n",
    "    logprobas = F.log_softmax(logits, dim=2)\n",
    "\n",
    "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
    "    actions_one_hot = to_one_hot(actions, n_actions).view(\n",
    "        actions.shape[0], actions.shape[1], n_actions)\n",
    "    logprobas_for_actions = torch.sum(logprobas * actions_one_hot, dim=-1)\n",
    "\n",
    "    # Now let's compute two loss components:\n",
    "    # 1) Policy gradient objective.\n",
    "    # Notes: Please don't forget to call .detach() on advantage term. Also please use mean, not sum.\n",
    "    # it's okay to use loops if you want\n",
    "    J_hat = 0  # policy objective as in the formula for J_hat\n",
    "\n",
    "    # 2) Temporal difference MSE for state values\n",
    "    # Notes: Please don't forget to call on V(s') term. Also please use mean, not sum.\n",
    "    # it's okay to use loops if you want\n",
    "    value_loss = 0\n",
    "\n",
    "    cumulative_returns = state_values[:, -1].detach()\n",
    "\n",
    "    for t in reversed(range(rollout_length)):\n",
    "        r_t = rewards[:, t]                                # current rewards\n",
    "        # current state values\n",
    "        V_t = state_values[:, t]\n",
    "        V_next = state_values[:, t + 1].detach()           # next state values\n",
    "        # log-probability of a_t in s_t\n",
    "        logpi_a_s_t = logprobas_for_actions[:, t]\n",
    "\n",
    "        # update G_t = r_t + gamma * G_{t+1} as we did in week6 reinforce\n",
    "        cumulative_returns = G_t = r_t + gamma * cumulative_returns\n",
    "\n",
    "        # Compute temporal difference error (MSE for V(s))\n",
    "        value_loss += <YOUR CODE >\n",
    "\n",
    "        # compute advantage A(s_t, a_t) using cumulative returns and V(s_t) as baseline\n",
    "        advantage = <YOUR CODE >\n",
    "        advantage = advantage.detach()\n",
    "\n",
    "        # compute policy pseudo-loss aka -J_hat.\n",
    "        J_hat += <YOUR CODE >\n",
    "\n",
    "    # regularize with entropy\n",
    "    entropy_reg = <compute entropy regularizer >\n",
    "\n",
    "    # add-up three loss components and average over time\n",
    "    loss = -J_hat / rollout_length +\\\n",
    "        value_loss / rollout_length +\\\n",
    "           -0.01 * entropy_reg\n",
    "\n",
    "    # Gradient descent step\n",
    "    < your code >\n",
    "\n",
    "    return loss.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's test it\n",
    "memory = list(pool.prev_memory_states)\n",
    "rollout_obs, rollout_actions, rollout_rewards, rollout_mask = pool.interact(10)\n",
    "\n",
    "train_on_rollout(rollout_obs, rollout_actions,\n",
    "                 rollout_rewards, rollout_mask, memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train \n",
    "\n",
    "just run train step and see if agent learns any better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from tqdm import trange\n",
    "from pandas import DataFrame\n",
    "moving_average = lambda x, **kw: DataFrame(\n",
    "    {'x': np.asarray(x)}).x.ewm(**kw).mean().values\n",
    "\n",
    "rewards_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in trange(15000):\n",
    "\n",
    "    memory = list(pool.prev_memory_states)\n",
    "    rollout_obs, rollout_actions, rollout_rewards, rollout_mask = pool.interact(\n",
    "        10)\n",
    "    train_on_rollout(rollout_obs, rollout_actions,\n",
    "                     rollout_rewards, rollout_mask, memory)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        rewards_history.append(np.mean(evaluate(agent, env, n_games=1)))\n",
    "        clear_output(True)\n",
    "        plt.plot(rewards_history, label='rewards')\n",
    "        plt.plot(moving_average(np.array(rewards_history),\n",
    "                                span=10), label='rewards ewma@10')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        if rewards_history[-1] >= 10000:\n",
    "            print(\"Your agent has just passed the minimum homework threshold\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relax and grab some refreshments while your agent is locked in an infinite loop of violence and death.\n",
    "\n",
    "__How to interpret plots:__\n",
    "\n",
    "The session reward is the easy thing: it should in general go up over time, but it's okay if it fluctuates ~~like crazy~~. It's also OK if it reward doesn't increase substantially before some 10k initial steps. However, if reward reaches zero and doesn't seem to get up over 2-3 evaluations, there's something wrong happening.\n",
    "\n",
    "\n",
    "Since we use a policy-based method, we also keep track of __policy entropy__ - the same one you used as a regularizer. The only important thing about it is that your entropy shouldn't drop too low (`< 0.1`) before your agent gets the yellow belt. Or at least it can drop there, but _it shouldn't stay there for long_.\n",
    "\n",
    "If it does, the culprit is likely:\n",
    "* Some bug in entropy computation. Remember that it is $ - \\sum p(a_i) \\cdot log p(a_i) $\n",
    "* Your agent architecture converges too fast. Increase entropy coefficient in actor loss. \n",
    "* Gradient explosion - just [clip gradients](https://stackoverflow.com/a/43486487) and maybe use a smaller network\n",
    "* Us. Or TF developers. Or aliens. Or lizardfolk. Contact us on forums before it's too late!\n",
    "\n",
    "If you're debugging, just run `logits, values = agent.step(batch_states)` and manually look into logits and values. This will reveal the problem 9 times out of 10: you'll likely see some NaNs or insanely large numbers or zeros. Try to catch the moment when this happens for the first time and investigate from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Final\" evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env_monitor = gym.wrappers.Monitor(env, directory=\"kungfu_videos\", force=True)\n",
    "final_rewards = evaluate(agent, env_monitor, n_games=20,)\n",
    "env_monitor.close()\n",
    "print(\"Final mean reward\", np.mean(final_rewards))\n",
    "\n",
    "video_names = list(filter(lambda s: s.endswith(\n",
    "    \".mp4\"), os.listdir(\"./kungfu_videos/\")))\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./kungfu_videos/\"+video_names[-1]))  # this may or may not be _last_ video. Try other indices"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
